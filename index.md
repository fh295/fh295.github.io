

I work on replicating in artificial systems how we as humans ***learn***, ***represent*** and ***use*** language and semantic concepts. 

### Embodied language learning

Children learn to understand language while learning to [perceive, interact-with, explain and make predictions about the world around them](http://psycnet.apa.org/record/1973-30971-000). Our linguistic knowledge depends critically on [sensory-motor and perceptual processes](https://www.tandfonline.com/doi/abs/10.1080/17470210701623605), which in turn are influenced and shaped by our language. My work simulates this process of acquiring language jointly with perceptual and motor processes as a path to realistic language understanding in fully embodied systems. With many brilliant collaborators at Deepmind, I have developed [agents that can learn the meaning of words and short phrases as they pertain to perceptual stimuli and complex action sequences]((https://arxiv.org/pdf/1706.06551.pdf)) in continuous [3D worlds](https://github.com/deepmind/lab). These agents naturally compose known words to successfully interpret never-seen-before phrases, a trait that matches the productivity of human language understanding. We also showed that learning is much more efficient if agents [exploit multiple complementary learning algorithms](https://arxiv.org/abs/1710.09867), another property of [human language learning](https://www.ncbi.nlm.nih.gov/pubmed/7624455).

### Perception and abstraction

A child might learn what growing means by observing a sibling, a pet or a plant get physically bigger, but once understood, the same idea of growing can be applied to pocket money, a tummy ache or Dad’s age. This ability to represent relations, principles or ideas like ‘growing’ with sufficient abstraction that they can be flexibly (re-)applied in disparate, and potentially unfamiliar, contexts and domains is [central to human cognition and language](https://www.youtube.com/watch?v=n8m7lFQ3njk). Our work studies how this ability [can be replicated in distributed learning systems like neural networks](https://arxiv.org/abs/1807.04225). 

### Language understanding from text

During my PhD, I worked with [Anna Korhonen](http://www.cl.cam.ac.uk/~alk23/) on ways to extract and represent meaning from text and other language data in distributed representations. I developed FastSent and Sequential Denoising Auto-Encoders, ways to learn [sentence representations from unlabelled text](http://www.aclweb.org/anthology/N16-1162). With [Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/) and [Kyunghyun Cho](http://www.kyunghyuncho.me/home), I noticed you can train a network on [dictionary definitions](http://www.aclweb.org/anthology/Q16-1002) to [solve general-knowledge crosswords clues](https://docs.google.com/gview?url=http://www.cl.cam.ac.uk/~fh295/crossword.pdf). With [Jase Weston](https://research.fb.com/people/weston-jason/) and [Antoine Bordes](https://research.fb.com/people/bordes-antoine/) I applied neural networks with external memory components to answer questions about [passages in books](https://arxiv.org/pdf/1511.02301.pdf). I also made [SimLex-999](http://www.cl.cam.ac.uk/~fh295/simlex.html "SimLex") a way to measure how well distributed representations reflect human semantic intuitions.  

### [Teaching](teaching.md)

With [Steve Clark](https://sites.google.com/site/stephenclark609/) I taught a Master's course [Deep Learning for NLP](teaching.md) at the Computer Laboratory, Cambridge University in 2018. If you follow that link you can find the synopsis, lecture slides and [Tensorflow code for training neural networks on dictionary definitions](https://github.com/fh295/Cambridge_DL4NLP). We got nice feedback, and hope to do the course again (somewhere) soon. 


### Recent talks

Kenote address, First [Mexican International Meeting on Artificial Intelligence](https://riiaa.org/speakers/).

Kenote address, 39th [TabuDag](https://www.let.rug.nl/tabudag/keynotes.php) meeting of linguists, Groningen, Netherlands. 

#### Other stuff

I started doing [Cognitive Science](https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12076) before I did any computational linguistics. And before that, I got a Master's in [pure maths](https://www.ox.ac.uk/admissions/undergraduate/courses-listing/mathematics-and-philosophy?wssl=1).

See [Google Scholar](https://scholar.google.com/citations?user=4HLUnhIAAAAJ&hl=en) for a list of publications.

When not working, things I like to do include football, running, yoga, [travelling](http://www.roadjunky.com/2078/backpackers-behind-bars-a-morning-in-quito-prison/) (but [not arriving](https://felix-india2009.blogspot.com/2009/06/paranoia-of-solo-travel.html)) and [relaxing](http://www.roadjunky.com/2087/zen-at-work-vipassana-an-indian-meditation-bootcamp/).


