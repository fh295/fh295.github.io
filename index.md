

I work on replicating in artificial systems how we as humans *learn*, *represent* and *use* language and semantic concepts in general. 

#### Embodied language learning

Children learn to understand language while learning to perceive, interact-with, explain and make predictions about the world around them. Our linguistic knowledge depends critically on sensory-motor and perceptual processes, which in turn are influenced and shaped by our language. My work simulates this process of acquiring language jointly with perceptual and motor processes as a path to realistic language understanding in fully embodied systems. With many brilliant collaborators at Deepmind, I have developed [agents that can learn](https://arxiv.org/pdf/1706.06551.pdf) the meaning of words and short phrases as they pertain to perceptual stimuli and complex action sequences in continuous 3D worlds. These agents naturally compose known words to successfully interpret never-seen-before phrases, a trait that matches the productivity of human language understanding. We also showed that learning is much more efficient if agents [exploit multiple complementary learning algorithms](https://arxiv.org/abs/1710.09867), another property of human language learning.

#### Perception and abstraction

A child might learn what growing means by observing a sibling, a pet or a plant get physically bigger, but once understood, the same idea of growing can be applied to pocket money, a tummy ache or Dad’s age. This ability to represent relations, principles or ideas like ‘growing’ with sufficient abstraction that they can be flexibly (re-)applied in disparate, and potentially unfamiliar, contexts and domains is [central to human cognition and language](https://www.youtube.com/watch?v=n8m7lFQ3njk).  [Our work](https://arxiv.org/abs/1807.04225) has shown that this ability can be replicated in distributed learning systems like neural networks, but that particular algorithms and training experience are required to induce representations of sufficient abstraction. 

#### Language understanding from textual and linguistic resources

During my PhD, I worked with [Anna Korhonen] on ways to extract and represent meaning from text and other language data in distributed representations. I developed FastSent and Sequential Denoising Auto-Encoders, ways to learn [sentence representations from unlabelled text](http://www.aclweb.org/anthology/N16-1162). With [Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/) and [Kyunghyun Cho](http://www.kyunghyuncho.me/home), I noticed you can train a network on [dictionary definitions](http://www.aclweb.org/anthology/Q16-1002) to [solve general-knowledge crosswords clues](https://docs.google.com/gview?url=http://www.cl.cam.ac.uk/~fh295/crossword.pdf). With [Jase Weston](https://research.fb.com/people/weston-jason/) and [Antoine Bordes](https://research.fb.com/people/bordes-antoine/) I applied neural networks with external memory components to answer questions about [passages in books](https://arxiv.org/pdf/1511.02301.pdf). I also made [SimLex-999](http://www.cl.cam.ac.uk/~fh295/simlex.html "SimLex") a way to measure how well distributed representations reflect human semantic intuitions.  

##### Other stuff

When not working, things I like to do include football, running, yoga, [travelling](http://www.roadjunky.com/2078/backpackers-behind-bars-a-morning-in-quito-prison/) (but [not arriving](https://felix-india2009.blogspot.com/2009/06/paranoia-of-solo-travel.html)) and [relaxing](http://www.roadjunky.com/2087/zen-at-work-vipassana-an-indian-meditation-bootcamp/).


